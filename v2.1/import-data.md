---
title: Import Data
summary: Learn how to import data into a CockroachDB cluster.
toc: true
---

CockroachDB supports importing data from CSV/TSV or SQL dump files.

{{site.data.alerts.callout_info}}To import/restore data from CockroachDB-generated <a href="backup.html">enterprise license backups</a>, see <a href="restore.html"><code>RESTORE</code></a>.{{site.data.alerts.end}}


## Import from tabular data (CSV)

If you have data exported in a tabular format (e.g., CSV or TSV), you can use the [`IMPORT`](import.html) statement.

To use this statement, though, you must also have some kind of remote file server (such as Amazon S3 or a custom file server) that all your nodes can access.

## Import from generic SQL dump

You can execute batches of `INSERT` statements stored in `.sql` files (including those generated by [`cockroach dump`](sql-dump.html)) from the command line, importing data into your cluster.

{% include copy-clipboard.html %}
~~~ shell
$ cockroach sql --database=[database name] < statements.sql
~~~

{{site.data.alerts.callout_success}}Grouping each <code>INSERT</code> statement to include approximately 500-10,000 rows will provide the best performance. The number of rows depends on row size, column families, number of indexes; smaller rows and less complex schemas can benefit from larger groups of <code>INSERTS</code>, while larger rows and more complex schemas benefit from smaller groups.{{site.data.alerts.end}}

## Import from PostgreSQL dump

{% include {{ page.version.version }}/misc/experimental-warning.md %}

<span class="version-tag">New in v2.1:</span> This section has instructions for getting data from PostgreSQL dump files into CockroachDB using [`IMPORT`](import.html). It uses the [employees data set](https://github.com/datacharmer/test_db) that is also used in the [MySQL docs](https://dev.mysql.com/doc/employee/en/).  The data set was imported into PostgreSQL from MySQL using [pgloader](https://pgloader.io).

- [Step 1. Dump the PostgreSQL database](#step-1-dump-the-postgresql-database)
- [Step 2. `IMPORT` the PostgreSQL dump file](#step-2-import-the-postgresql-dump-file)

### Step 1. Dump the PostgreSQL database

To dump the `employees` table from a PostgreSQL database also named `employees`, run the [`pg_dump`](https://www.postgresql.org/docs/current/static/app-pgdump.html) command below.

{% include copy-clipboard.html %}
~~~ shell
$ pg_dump --inserts --no-privileges -t employees --no-owner employees > /tmp/employees.sql
~~~

The following options are required:

 Option            | Description
-------------------+--------------------------------------------------------------------------------------------------------------------------------------
 `--inserts`       | Required until CockroachDB implements support for the PostgreSQL [`COPY`](https://www.postgresql.org/docs/current/static/sql-copy.html) statement.
 `--no-owner`      | Required until CockroachDB implements support for PostgreSQL's object ownership statements.
 `--no-privileges` | Required until CockroachDB implements support for PostgreSQL's access privilege statements.

### Step 2. `IMPORT` the PostgreSQL dump file

You will need to look at the dump file and translate the PostgreSQL [`CREATE TABLE`](create-table.html) statement there into an [`IMPORT TABLE`](import.html) statement CockroachDB understands.

For this data set, the PostgreSQL dump file required one edit: the type of the `gender` column had to be changed from `employees.employees_gender` to [`STRING`](string.html) since PostgreSQL represented the employee's gender using a [`CREATE TYPE`](https://www.postgresql.org/docs/10/static/sql-createtype.html) statement that is not yet supported by CockroachDB. If you'd rather not edit the PostgreSQL dump file by hand, you can run the following command (which was only tested for this example):

{% include copy-clipboard.html %}
~~~ shell
$ perl -i.bak -lapE 's/gender employees\.employees_gender/gender STRING/' employees.sql
~~~

This example uses S3. For a complete list of the types of cloud storage [`IMPORT`](import.html) can pull from, see [Import File URLs](import.html#import-file-urls).

To import the PostgreSQL dump file, issue an `IMPORT` statement like the one below.

{% include copy-clipboard.html %}
~~~ sql
> IMPORT
TABLE
  employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name STRING NOT NULL,
    last_name STRING NOT NULL,
    gender STRING NOT NULL,
    hire_date DATE NOT NULL
  )
PGDUMP
  DATA (
    's3://your-external-storage/employees.sql?AWS_ACCESS_KEY_ID=ACCESSKEY&AWS_SECRET_ACCESS_KEY=SECRET'
  );
~~~

Success will look like:

~~~
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
|       job_id       |  status   | fraction_completed | rows | index_entries | system_records | bytes |
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
| 352938237301293057 | succeeded |                  1 |    0 |             0 |              0 |     0 |
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
(1 row)
~~~

Finally, confirm that despite needing to edit the `gender` column's data type in the PostgreSQL dump file, the column was populated correctly during the import:

{% include copy-clipboard.html %}
~~~ sql
SELECT * FROM employees LIMIT 10;
~~~

~~~
+--------+---------------------------+------------+-----------+--------+---------------------------+
| emp_no |        birth_date         | first_name | last_name | gender |         hire_date         |
+--------+---------------------------+------------+-----------+--------+---------------------------+
|  10001 | 1953-09-02 00:00:00+00:00 | Georgi     | Facello   | M      | 1986-06-26 00:00:00+00:00 |
|  10002 | 1964-06-02 00:00:00+00:00 | Bezalel    | Simmel    | F      | 1985-11-21 00:00:00+00:00 |
|  10003 | 1959-12-03 00:00:00+00:00 | Parto      | Bamford   | M      | 1986-08-28 00:00:00+00:00 |
|  10004 | 1954-05-01 00:00:00+00:00 | Chirstian  | Koblick   | M      | 1986-12-01 00:00:00+00:00 |
|  10005 | 1955-01-21 00:00:00+00:00 | Kyoichi    | Maliniak  | M      | 1989-09-12 00:00:00+00:00 |
|  10006 | 1953-04-20 00:00:00+00:00 | Anneke     | Preusig   | F      | 1989-06-02 00:00:00+00:00 |
|  10007 | 1957-05-23 00:00:00+00:00 | Tzvetan    | Zielinski | F      | 1989-02-10 00:00:00+00:00 |
|  10008 | 1958-02-19 00:00:00+00:00 | Saniya     | Kalloufi  | M      | 1994-09-15 00:00:00+00:00 |
|  10009 | 1952-04-19 00:00:00+00:00 | Sumant     | Peac      | F      | 1985-02-18 00:00:00+00:00 |
|  10010 | 1963-06-01 00:00:00+00:00 | Duangkaew  | Piveteau  | F      | 1989-08-24 00:00:00+00:00 |
+--------+---------------------------+------------+-----------+--------+---------------------------+
(10 rows)
~~~

To load the data from local files, you have 2 options:

- Start up a [local HTTP server](create-a-file-server.html#using-caddy-as-a-file-server) to serve the files, and point `IMPORT ... PGDUMP` at the server's URL. This is the recommended option.

- Create an `extern` subdirectory on each node and copy the PostgreSQL dump files there. Note that you will need to copy the dump files onto every node, since CockroachDB may execute the `IMPORT` statement on any of the nodes.

If you decide to load the data from the `extern` subdirectory, you will need to use [`IMPORT`'s `nodelocal` URL scheme](import.html#import-file-urls) as shown below.

{% include copy-clipboard.html %}
~~~ sql
> IMPORT
TABLE
  employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name STRING NOT NULL,
    last_name STRING NOT NULL,
    gender STRING NOT NULL,
    hire_date DATE NOT NULL
  )
PGDUMP
  DATA ('nodelocal:///employees_table.sql');
~~~

## Import from MySQL dump

{% include {{ page.version.version }}/misc/experimental-warning.md %}

<span class="version-tag">New in v2.1:</span> This section has instructions for getting data from MySQL dump files into CockroachDB using [`IMPORT`](import.html). It uses the [employees data set](https://github.com/datacharmer/test_db) that is also used in the [MySQL docs](https://dev.mysql.com/doc/employee/en/).

- [Step 1. Dump the MySQL database](#step-1-dump-the-mysql-database)
- [Step 2. `IMPORT` the dump files](#step-2-import-the-mysql-dump-files)

### Step 1. Dump the MySQL database

{% include copy-clipboard.html %}
~~~ shell
$ mysqldump -uroot employees employees > employees.sql
~~~

### Step 2. `IMPORT` the MySQL dump files

To import the MySQL dump file for a table, issue an `IMPORT` statement like the one shown below.

You will need to look at the dump file and translate the MySQL [`CREATE TABLE`](create-table.html) statement there into something CockroachDB understands.

This example uses S3. For a complete list of the types of cloud storage `IMPORT` can pull from, see [Import File URLs](import.html#import-file-urls).

{% include copy-clipboard.html %}
~~~ sql
> IMPORT
TABLE
  employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name STRING NOT NULL,
    last_name STRING NOT NULL,
    gender STRING NOT NULL,
    hire_date DATE NOT NULL
  )
MYSQLDUMP
  DATA (
    's3://your-external-storage/employees.sql?AWS_ACCESS_KEY_ID=ACCESSKEY&AWS_SECRET_ACCESS_KEY=SECRET'
  );
~~~

Success will look like:

~~~
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
|       job_id       |  status   | fraction_completed | rows | index_entries | system_records | bytes |
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
| 352938237301293057 | succeeded |                  1 |    0 |             0 |              0 |     0 |
+--------------------+-----------+--------------------+------+---------------+----------------+-------+
(1 row)
~~~

To load the data from local files, you have 2 options:

- Start up a [local HTTP server](create-a-file-server.html#using-caddy-as-a-file-server) to serve the files, and point `MYSQLDUMP` at the server's URL. This is the recommended option.

- Create an `extern` subdirectory on each node and copy the MySQL dump files there. Note that you will need to copy the dump files onto every node, since CockroachDB may execute the `IMPORT` statement on any of the nodes.

If you decide to load the data from the `extern` subdirectory, you will need to use [`IMPORT`'s `nodelocal` URL scheme](import.html#import-file-urls) as shown below.

{% include copy-clipboard.html %}
~~~ sql
> IMPORT
TABLE
  employees (
    emp_no INT PRIMARY KEY,
    birth_date DATE NOT NULL,
    first_name STRING NOT NULL,
    last_name STRING NOT NULL,
    gender STRING NOT NULL,
    hire_date DATE NOT NULL
  )
MYSQLDUMP
  DATA ('nodelocal:///employees_table.sql');
~~~

## See also

- [`IMPORT`](import.html)
- [SQL Dump (Export)](sql-dump.html)
- [Backup and Restore Data](backup-and-restore.html)
- [Use the Built-in SQL Client](use-the-built-in-sql-client.html)
- [Other Cockroach Commands](cockroach-commands.html)
